{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdd0d9f9e89a274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:29.025519Z",
     "start_time": "2023-11-06T17:28:25.326436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6f43b2f2931c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dd37d16e1f625a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T11:22:03.764512Z",
     "start_time": "2023-11-08T11:22:03.742551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHECK BEFORE RUNNING!!!\n",
    "flag_preprocess = False # If false, skip the generation of a preprocessed corpus\n",
    "flag_cluster_labels = False \n",
    "flag_cluster = False # If false, stop overwrite of a cluster matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d135c63f2f59940a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:32.804694Z",
     "start_time": "2023-11-06T17:28:32.384608Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define column names for movie.metadata.tsv\n",
    "movie_metadata_columns = [\"Wikipedia Movie ID\", \"Freebase Movie ID\", \"Movie name\", \"Movie release date\",\n",
    "                          \"Movie box office revenue\", \"Movie runtime\", \"Movie languages\", \"Movie countries\",\n",
    "                          \"Movie genres\"]\n",
    "\n",
    "# Read movie.metadata.tsv into a DataFrame\n",
    "movie_metadata_df = pd.read_csv(\"data/movie.metadata.tsv\", sep='\\t', header=None, names=movie_metadata_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:36.293482Z",
     "start_time": "2023-11-06T17:28:33.574171Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mathiaskroismoller/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mathiaskroismoller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    # Convert to lowercase and tokenize\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Open 'plot_summaries.txt'\n",
    "with open('data/plot_summaries.txt', 'r', encoding='utf-8') as file:\n",
    "    summaries = file.readlines()\n",
    "\n",
    "# Extract movie IDs \n",
    "movie_ids = [int(summary.split()[0]) for summary in summaries]\n",
    "\n",
    "# Preprocess the summaries \n",
    "preprocessed_summaries = []\n",
    "if flag_preprocess:\n",
    "    for summary in tqdm(summaries, desc=\"Processing summaries\"):\n",
    "        preprocessed_summaries.append(preprocess_text(summary.split('\\t')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70280c4bbcfe17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:38.248747Z",
     "start_time": "2023-11-06T17:28:38.209321Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save preprocessed summaries to a pkl file\n",
    "import csv\n",
    "if flag_preprocess:\n",
    "    # Save preprocessed_summaries list to a file\n",
    "    with open('preprocessed_summaries.pkl', 'wb') as file:\n",
    "        pickle.dump(preprocessed_summaries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbca78b926507c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:39.158115Z",
     "start_time": "2023-11-06T17:28:39.126012Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combined stemmed summaries with labels \n",
    "summaries_preprocessed_dist = []\n",
    "if flag_preprocess:\n",
    "    for i in range(len(preprocessed_summaries)):\n",
    "        summaries_preprocessed_dist.append({\"id\": movie_ids[i], \"text\": preprocessed_summaries[i]})\n",
    "    \n",
    "    # Save summaries_preprocessed_dist to a JSON file\n",
    "    with open('summaries_preprocessed_dist.json', 'w') as f:\n",
    "        json.dump(summaries_preprocessed_dist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2818cdebf047",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Home made version TF/IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7224696ad5a9625f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:28:50.030479Z",
     "start_time": "2023-11-06T17:28:49.738404Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load preprocessed_summaries list from the saved file\n",
    "with open('preprocessed_summaries.pkl', 'rb') as file:\n",
    "    preprocessed_summaries = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48432b3e10fdfcee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:29:00.873846Z",
     "start_time": "2023-11-06T17:28:53.174567Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9677733f4d61f5d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T18:00:41.223036Z",
     "start_time": "2023-11-06T17:29:15.896575Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Apply KMeans clustering\n",
    "if flag_cluster_labels:\n",
    "    num_clusters = 100  # You can adjust the number of clusters -> the more clusters the heavier the computation\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix) # This takes a loooong time...\n",
    "    \n",
    "    # Save cluster_labels to a file\n",
    "    with open('cluster_labels.pkl', 'wb') as f:\n",
    "        pickle.dump(cluster_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95def09953470139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T18:56:05.045989Z",
     "start_time": "2023-11-06T18:56:04.975355Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load cluster_labels from a file\n",
    "with open('cluster_labels.pkl', 'rb') as f:\n",
    "    cluster_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35ddaa5a5d1cdda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T10:58:58.297654Z",
     "start_time": "2023-11-08T10:58:58.101458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store movie IDs and their corresponding cluster labels\n",
    "movie_clusters = dict(zip(movie_ids, cluster_labels))\n",
    "\n",
    "# Save the dictionary to a file using pickle\n",
    "with open('movie_clusters_id.pkl', 'wb') as file:\n",
    "    pickle.dump(movie_clusters, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f62ba84723dfac33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T11:19:19.984244Z",
     "start_time": "2023-11-08T10:59:10.161009Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42306/42306 [20:07<00:00, 35.04it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store cluster information\n",
    "cluster_df = pd.DataFrame(columns=[f'Cluster {i}' for i in range(num_clusters)])\n",
    "\n",
    "# Iterate through movie clusters and match with movie_metadata_df\n",
    "if flag_cluster:\n",
    "    for movie_id, cluster_label in tqdm(movie_clusters.items()):\n",
    "        # Find the corresponding row in movie_metadata_df based on Wikipedia Movie ID\n",
    "        movie_row = movie_metadata_df[movie_metadata_df['Wikipedia Movie ID'] == movie_id]\n",
    "        \n",
    "        # Extract movie name from the row\n",
    "        movie_name = movie_row['Movie name'].values[0] if not movie_row.empty else f'Movie {movie_id}'\n",
    "        \n",
    "        # Update cluster_df with movie name in the appropriate cluster column\n",
    "        cluster_df.loc[len(cluster_df), f'Cluster {cluster_label}'] = movie_name\n",
    "    \n",
    "    \n",
    "    # Fill NaN values with empty string for better representation\n",
    "    cluster_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "595156f2c8eb5278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T11:21:21.119975Z",
     "start_time": "2023-11-08T11:21:19.152167Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save cluster_df as CSV\n",
    "if flag_cluster:\n",
    "    cluster_df.to_csv('clustered_movies.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6d4f84dbd7e9493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T17:04:50.297616Z",
     "start_time": "2023-11-07T17:04:49.646818Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hg/tvll5hmx5b580xjtn0kmrff00000gn/T/ipykernel_5219/4017522519.py:2: DtypeWarning: Columns (31,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cluster_df = pd.read_csv('clustered_movies.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load cluster_df from a file\n",
    "cluster_df = pd.read_csv('clustered_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592a009517e4f14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Top 10 words pr cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9480f3fbe8cc48b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_words_for_clusters(tfidf_matrix, cluster_labels, vectorizer, top_n=10):\n",
    "    cluster_words = {}\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for cluster_label in range(max(cluster_labels) + 1):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_label)[0]\n",
    "        cluster_tfidf_scores = np.sum(tfidf_matrix[cluster_indices], axis=0)\n",
    "        sorted_indices = np.argsort(cluster_tfidf_scores)[0, ::-1][:top_n]\n",
    "        top_words = [terms[i] for i in sorted_indices]\n",
    "        cluster_words[cluster_label] = top_words\n",
    "        \n",
    "    return cluster_words\n",
    "\n",
    "# Call the function to get cluster words\n",
    "cluster_words = get_top_words_for_clusters(tfidf_matrix, cluster_labels, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4e31bc29d96ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:48:10.558176Z",
     "start_time": "2023-11-07T15:48:10.379355Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['peter', 'love', 'man', 'wendy', 'spider', 'father', 'jack',\n",
       "       'back', 'home', 'one'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_words[0][0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a41d7d837d72e0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:59:31.812204Z",
     "start_time": "2023-11-07T15:59:31.600599Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peter</td>\n",
       "      <td>jesse</td>\n",
       "      <td>raju</td>\n",
       "      <td>school</td>\n",
       "      <td>henry</td>\n",
       "      <td>cite</td>\n",
       "      <td>agent</td>\n",
       "      <td>bill</td>\n",
       "      <td>one</td>\n",
       "      <td>love</td>\n",
       "      <td>...</td>\n",
       "      <td>vijay</td>\n",
       "      <td>jim</td>\n",
       "      <td>charles</td>\n",
       "      <td>joan</td>\n",
       "      <td>man</td>\n",
       "      <td>sylvester</td>\n",
       "      <td>harry</td>\n",
       "      <td>martin</td>\n",
       "      <td>bob</td>\n",
       "      <td>creature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>james</td>\n",
       "      <td>karan</td>\n",
       "      <td>students</td>\n",
       "      <td>anne</td>\n",
       "      <td>web</td>\n",
       "      <td>president</td>\n",
       "      <td>new</td>\n",
       "      <td>police</td>\n",
       "      <td>falls</td>\n",
       "      <td>...</td>\n",
       "      <td>love</td>\n",
       "      <td>silver</td>\n",
       "      <td>katie</td>\n",
       "      <td>joseph</td>\n",
       "      <td>young</td>\n",
       "      <td>tweety</td>\n",
       "      <td>one</td>\n",
       "      <td>police</td>\n",
       "      <td>larry</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "      <td>willy</td>\n",
       "      <td>amar</td>\n",
       "      <td>high</td>\n",
       "      <td>catherine</td>\n",
       "      <td>synopsis</td>\n",
       "      <td>united</td>\n",
       "      <td>life</td>\n",
       "      <td>two</td>\n",
       "      <td>marriage</td>\n",
       "      <td>...</td>\n",
       "      <td>ravi</td>\n",
       "      <td>father</td>\n",
       "      <td>edward</td>\n",
       "      <td>jack</td>\n",
       "      <td>old</td>\n",
       "      <td>cat</td>\n",
       "      <td>voldemort</td>\n",
       "      <td>wife</td>\n",
       "      <td>kevin</td>\n",
       "      <td>godzilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wendy</td>\n",
       "      <td>ethan</td>\n",
       "      <td>vicky</td>\n",
       "      <td>teacher</td>\n",
       "      <td>sir</td>\n",
       "      <td>film</td>\n",
       "      <td>states</td>\n",
       "      <td>connie</td>\n",
       "      <td>life</td>\n",
       "      <td>girl</td>\n",
       "      <td>...</td>\n",
       "      <td>kumar</td>\n",
       "      <td>new</td>\n",
       "      <td>bella</td>\n",
       "      <td>life</td>\n",
       "      <td>one</td>\n",
       "      <td>granny</td>\n",
       "      <td>two</td>\n",
       "      <td>family</td>\n",
       "      <td>song</td>\n",
       "      <td>frankenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spider</td>\n",
       "      <td>celine</td>\n",
       "      <td>love</td>\n",
       "      <td>student</td>\n",
       "      <td>family</td>\n",
       "      <td>based</td>\n",
       "      <td>soviet</td>\n",
       "      <td>film</td>\n",
       "      <td>new</td>\n",
       "      <td>daughter</td>\n",
       "      <td>...</td>\n",
       "      <td>father</td>\n",
       "      <td>stifler</td>\n",
       "      <td>sir</td>\n",
       "      <td>father</td>\n",
       "      <td>wife</td>\n",
       "      <td>speedy</td>\n",
       "      <td>man</td>\n",
       "      <td>blomkvist</td>\n",
       "      <td>however</td>\n",
       "      <td>dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>staff</td>\n",
       "      <td>moving</td>\n",
       "      <td>sell</td>\n",
       "      <td>yuko</td>\n",
       "      <td>shakespeare</td>\n",
       "      <td>classic</td>\n",
       "      <td>opens</td>\n",
       "      <td>ever</td>\n",
       "      <td>youth</td>\n",
       "      <td>aristocrat</td>\n",
       "      <td>...</td>\n",
       "      <td>birth</td>\n",
       "      <td>cop</td>\n",
       "      <td>ceremony</td>\n",
       "      <td>encounter</td>\n",
       "      <td>jealous</td>\n",
       "      <td>vacation</td>\n",
       "      <td>feet</td>\n",
       "      <td>words</td>\n",
       "      <td>detective</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>lomax</td>\n",
       "      <td>radio</td>\n",
       "      <td>koya</td>\n",
       "      <td>post</td>\n",
       "      <td>accepts</td>\n",
       "      <td>sik</td>\n",
       "      <td>especially</td>\n",
       "      <td>honesty</td>\n",
       "      <td>pass</td>\n",
       "      <td>satya</td>\n",
       "      <td>...</td>\n",
       "      <td>indeed</td>\n",
       "      <td>eye</td>\n",
       "      <td>quickly</td>\n",
       "      <td>famine</td>\n",
       "      <td>dave</td>\n",
       "      <td>exhibit</td>\n",
       "      <td>pc</td>\n",
       "      <td>engineer</td>\n",
       "      <td>tree</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>mcnally</td>\n",
       "      <td>pull</td>\n",
       "      <td>millionaire</td>\n",
       "      <td>reluctant</td>\n",
       "      <td>befriends</td>\n",
       "      <td>murderer</td>\n",
       "      <td>married</td>\n",
       "      <td>goldie</td>\n",
       "      <td>program</td>\n",
       "      <td>present</td>\n",
       "      <td>...</td>\n",
       "      <td>motley</td>\n",
       "      <td>protecting</td>\n",
       "      <td>neighbor</td>\n",
       "      <td>dinner</td>\n",
       "      <td>action</td>\n",
       "      <td>holding</td>\n",
       "      <td>elevator</td>\n",
       "      <td>sexual</td>\n",
       "      <td>search</td>\n",
       "      <td>liquid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>nurse</td>\n",
       "      <td>shadow</td>\n",
       "      <td>microdot</td>\n",
       "      <td>naomi</td>\n",
       "      <td>realize</td>\n",
       "      <td>train</td>\n",
       "      <td>interviews</td>\n",
       "      <td>jackie</td>\n",
       "      <td>miller</td>\n",
       "      <td>american</td>\n",
       "      <td>...</td>\n",
       "      <td>kallu</td>\n",
       "      <td>suddenly</td>\n",
       "      <td>health</td>\n",
       "      <td>bloodthirsty</td>\n",
       "      <td>stage</td>\n",
       "      <td>mission</td>\n",
       "      <td>states</td>\n",
       "      <td>highwaymen</td>\n",
       "      <td>rob</td>\n",
       "      <td>weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>date</td>\n",
       "      <td>inform</td>\n",
       "      <td>hates</td>\n",
       "      <td>kyle</td>\n",
       "      <td>summer</td>\n",
       "      <td>body</td>\n",
       "      <td>tech</td>\n",
       "      <td>roper</td>\n",
       "      <td>woods</td>\n",
       "      <td>approve</td>\n",
       "      <td>...</td>\n",
       "      <td>visits</td>\n",
       "      <td>person</td>\n",
       "      <td>property</td>\n",
       "      <td>uncredited</td>\n",
       "      <td>amusement</td>\n",
       "      <td>hearing</td>\n",
       "      <td>cedric</td>\n",
       "      <td>thief</td>\n",
       "      <td>bar</td>\n",
       "      <td>joe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1            2          3            4         5  \\\n",
       "0      peter   jesse         raju     school        henry      cite   \n",
       "1       love   james        karan   students         anne       web   \n",
       "2        man   willy         amar       high    catherine  synopsis   \n",
       "3      wendy   ethan        vicky    teacher          sir      film   \n",
       "4     spider  celine         love    student       family     based   \n",
       "..       ...     ...          ...        ...          ...       ...   \n",
       "995    staff  moving         sell       yuko  shakespeare   classic   \n",
       "996    lomax   radio         koya       post      accepts       sik   \n",
       "997  mcnally    pull  millionaire  reluctant    befriends  murderer   \n",
       "998    nurse  shadow     microdot      naomi      realize     train   \n",
       "999     date  inform        hates       kyle       summer      body   \n",
       "\n",
       "              6        7        8           9  ...      90          91  \\\n",
       "0         agent     bill      one        love  ...   vijay         jim   \n",
       "1     president      new   police       falls  ...    love      silver   \n",
       "2        united     life      two    marriage  ...    ravi      father   \n",
       "3        states   connie     life        girl  ...   kumar         new   \n",
       "4        soviet     film      new    daughter  ...  father     stifler   \n",
       "..          ...      ...      ...         ...  ...     ...         ...   \n",
       "995       opens     ever    youth  aristocrat  ...   birth         cop   \n",
       "996  especially  honesty     pass       satya  ...  indeed         eye   \n",
       "997     married   goldie  program     present  ...  motley  protecting   \n",
       "998  interviews   jackie   miller    american  ...   kallu    suddenly   \n",
       "999        tech    roper    woods     approve  ...  visits      person   \n",
       "\n",
       "           92            93         94         95         96          97  \\\n",
       "0     charles          joan        man  sylvester      harry      martin   \n",
       "1       katie        joseph      young     tweety        one      police   \n",
       "2      edward          jack        old        cat  voldemort        wife   \n",
       "3       bella          life        one     granny        two      family   \n",
       "4         sir        father       wife     speedy        man   blomkvist   \n",
       "..        ...           ...        ...        ...        ...         ...   \n",
       "995  ceremony     encounter    jealous   vacation       feet       words   \n",
       "996   quickly        famine       dave    exhibit         pc    engineer   \n",
       "997  neighbor        dinner     action    holding   elevator      sexual   \n",
       "998    health  bloodthirsty      stage    mission     states  highwaymen   \n",
       "999  property    uncredited  amusement    hearing     cedric       thief   \n",
       "\n",
       "            98            99  \n",
       "0          bob      creature  \n",
       "1        larry       monster  \n",
       "2        kevin      godzilla  \n",
       "3         song  frankenstein  \n",
       "4      however        dragon  \n",
       "..         ...           ...  \n",
       "995  detective         close  \n",
       "996       tree          cold  \n",
       "997     search        liquid  \n",
       "998        rob       weapons  \n",
       "999        bar           joe  \n",
       "\n",
       "[1000 rows x 100 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe for top 1000 words ranked for every cluster\n",
    "cluster_words_ranked = pd.DataFrame()\n",
    "\n",
    "for i in range(len(cluster_words)):\n",
    "    cluster_words_ranked[f'{i}'] = cluster_words[i][0][0][:1000]\n",
    "    \n",
    "cluster_words_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56bbdec4dab76738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:59:34.064003Z",
     "start_time": "2023-11-07T15:59:33.996034Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_words_ranked.to_csv('cluster_words.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e33cc2b984c4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NPL version (Nothing yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e50c55d094624b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T16:11:39.091150Z",
     "start_time": "2023-11-06T16:11:38.911288Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load summaries_preprocessed_dist from a JSON file\n",
    "with open('summaries_preprocessed_dist.json', 'r') as f:\n",
    "    summaries_preprocessed_dist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10b04b346fe708ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T15:50:38.847465Z",
     "start_time": "2023-11-06T15:50:38.263592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b8fa341c2ad36bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T15:51:12.353947Z",
     "start_time": "2023-11-06T15:51:12.322750Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2f8516f8486b447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T16:12:47.846330Z",
     "start_time": "2023-11-06T16:12:47.610183Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_sum \u001b[38;5;241m=\u001b[39m \u001b[43msummaries_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "tokenized_sum = summaries_preprocessed_dist.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e082786ad5e1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Enriching data\n",
    "#### 1. Adding data from www.themoviedb.org (TMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36fd8944cec23f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section we are going to be using the API of www.themoviedb.org (TMDB) to populate the existing dataframe with more information about the movie. The script is going to query the website with the movie name and match with the movie, where the release date is the closest. If no match is found the code skips the movie and goes to the next.\n",
    "\n",
    "This 1. step is most importantly adding the TMDB_ID which is a unique identifier for the movie which can be used to further enrich the data.\n",
    "\n",
    "This script was run separately in another file in /scratch/add_TMDB_movie_metadata.py as it needed to be run several times since the API was limited by 40 requests/s.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fdcaa618d558119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T15:59:23.062785300Z",
     "start_time": "2023-11-06T15:59:22.985435200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/movie.metadata.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m load_dotenv() \u001b[38;5;66;03m# Loads .env files aka. the TMDB_API_KEY\u001b[39;00m\n\u001b[0;32m     10\u001b[0m headers_movie_metadata \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipedia Movie ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFreebase Movie ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie release date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie box office revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie languages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie countries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie genres\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m movie_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/movie.metadata.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39mheaders_movie_metadata)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load API key\u001b[39;00m\n\u001b[0;32m     17\u001b[0m TMDB_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTMDB_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/movie.metadata.tsv'"
     ]
    }
   ],
   "source": [
    "import tmdbsimple as tmdb # Wrapper library for the API of themoviedb.org  (TMDB)\n",
    "from tqdm import tqdm # Progress bar for the script\n",
    "import pandas as pd\n",
    "from datetime import datetime # Used for comparing movie release dates\n",
    "from dotenv import load_dotenv # Makes keeping the API-key as local environment file simpler\n",
    "import os # Used for loading the .env file\n",
    "\n",
    "load_dotenv() # Loads .env files aka. the TMDB_API_KEY\n",
    "\n",
    "headers_movie_metadata = [\"Wikipedia Movie ID\", \"Freebase Movie ID\", \"Movie name\", \"Movie release date\",\n",
    "                          \"Movie box office revenue\", \"Movie runtime\", \"Movie languages\", \"Movie countries\",\n",
    "                          \"Movie genres\"]\n",
    "movie_metadata = pd.read_csv('/data/movie.metadata.tsv', sep=\"\\t\", names=headers_movie_metadata)\n",
    "\n",
    "\n",
    "# Load API key\n",
    "TMDB_API_KEY = os.environ.get(\"TMDB_API_KEY\")\n",
    "tmdb.API_KEY = TMDB_API_KEY\n",
    "tmdb.REQUESTS_TIMEOUT = 5  # Seconds, for both connect and read\n",
    "\n",
    "# Create a list to save progress\n",
    "saved_progress = []\n",
    "\n",
    "# Determine where to resume\n",
    "start_index = 28000\n",
    "\n",
    "# Progress file that can be used to resume\n",
    "#saved_progress = pd.read_json('progress.json')['index'].tolist()\n",
    "#start_index = saved_progress[-1] + 1  # Start from the next index\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "movie_metadata_TMDB = movie_metadata.copy()\n",
    "\n",
    "for index, row in tqdm(movie_metadata_TMDB.iterrows(), total=len(movie_metadata_TMDB), desc=\"Processing\"): # Wraps for loop in progress bar.\n",
    "    \n",
    "    # Skip previously processed indices - Commented out in favor of manual start_index\n",
    "    #if index in saved_progress:\n",
    "    #    continue\n",
    "    if index < start_index:\n",
    "        continue\n",
    "    try:\n",
    "        if not pd.isna(row[\"Movie release date\"]):\n",
    "            search = tmdb.Search()\n",
    "            response = search.movie(query=row[\"Movie name\"])\n",
    "\n",
    "            # Convert dataframe release date to datetime\n",
    "            movie_release_date_str = row[\"Movie release date\"]\n",
    "            if len(movie_release_date_str) == 4:  # Handle \"YYYY\" format\n",
    "                movie_release_date = datetime.strptime(movie_release_date_str, \"%Y\").date()\n",
    "            elif len(movie_release_date_str) == 7:  # Handle \"YYYY-DD\" format\n",
    "                movie_release_date = datetime.strptime(movie_release_date_str, \"%Y-%m\").date()\n",
    "            else:  # Assume it's in the format \"YYYY-MM-DD\"\n",
    "                movie_release_date = datetime.strptime(movie_release_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "            # Handle if release_date is empty.\n",
    "            date_list_converted = [datetime.strptime(each_date['release_date'], \"%Y-%m-%d\").date() for each_date in\n",
    "                                   search.results if each_date.get('release_date')]\n",
    "\n",
    "            # Create list of differences in time\n",
    "            differences = [abs(movie_release_date - each_date) for each_date in date_list_converted]\n",
    "            \n",
    "            # If differences are empty = skip\n",
    "            if not differences:\n",
    "                continue\n",
    "            minimum_index = differences.index(min(differences))  # Index of the closest match\n",
    "            match = search.results[minimum_index]\n",
    "            # print(f\"Closest match: {match['title']} (Release Date: {match['release_date']})\")\n",
    "\n",
    "            # Add info in dataframe about the movie\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_id'] = match['id']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_original_language'] = match['original_language']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_original_title'] = match['original_title']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_overview'] = match['overview']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_popularity'] = match['popularity']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_release_date'] = match['release_date']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_title'] = match['title']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_vote_average'] = match['vote_average']\n",
    "            movie_metadata_TMDB.loc[index, 'TMDB_vote_count'] = match['vote_count']\n",
    "            \n",
    "            # Save the index as progress\n",
    "            saved_progress.append(index)\n",
    "\n",
    "            # Save progress periodically (in case of interruption)\n",
    "            if index % 50 == 0:\n",
    "                progress_df = pd.DataFrame({'index': saved_progress})\n",
    "                progress_df.to_json('progress.json')\n",
    "                movie_metadata_TMDB.to_csv('movie_metadata_TMDB.csv', index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {index}: {str(e)}\")\n",
    "\n",
    "# Save final progress\n",
    "progress_df = pd.DataFrame({'index': saved_progress})\n",
    "progress_df.to_json('progress.json')\n",
    "\n",
    "# Save your final DataFrame\n",
    "movie_metadata_TMDB.to_csv('modified_data/movie_metadata_TMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162d1f169e7b3be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T16:00:39.513370200Z",
     "start_time": "2023-11-06T16:00:38.240990600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added TMDB ID to 68944 movies. Total movies: 81741. \n",
      " Percentage populated: 84.34%\n"
     ]
    }
   ],
   "source": [
    "# Comparing how many rows were populated in the enriching.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('modified_data/movie_metadata_TMDB.csv')\n",
    "print(f\"Added TMDB ID to {df['TMDB_id'].count()} movies. Total movies: {df['Wikipedia Movie ID'].count()}. \\n Percentage populated: {round(df['TMDB_id'].count()/df['Wikipedia Movie ID'].count()*100,2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115afda8f5a0740",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Adding Movie Revenue\n",
    "Many of the movies are missing their revenue and TMDB has that information.\n",
    "E.g.: https://www.themoviedb.org/movie/51284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40cc9605485cdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T16:07:08.695117700Z",
     "start_time": "2023-11-06T16:07:08.676047700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a68eabba0f92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c254840e0c90d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
